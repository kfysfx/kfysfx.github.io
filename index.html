<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="Video Event Detection" />
	<meta name="keywords" content="Constraint Flow" />	
	<meta name="author" content="Wei Zhen" />
	<link rel="stylesheet" href="css/SPR.css" type="text/css" />
	<title>Semantic Preserving Retargeting</title>
</head>


<body>


<!-- --------------------------------
-
- header
-
---- --------------------------------
-->  
	<div id="header">
		<div class="wrap">
		  <div id="intro">
		    	<h1 align="center" id="logo">Semantic Preserving Retargeting</h1>
		      <div align="center">
			      <table width="80%" border="0" align="center" cellpadding="0" cellspacing="0">
			        <tr>
			          <td width="25%" height="30" align='center'><a target="_blank">Si Liu<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Zhen Wei<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Yao Sun<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Xinyu Ou<sup>1,2,3</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Junyu Lin<sup>1</sup></a></td>
		            </tr>
		        </table>
		        <table>
		            <tr>
		              <td width="25%" height="30" align='center'><a target="_blank">Bin Liu<sup>4</sup></a></td>
					  <td width="25%" height="30" align='center'><a target="_blank">Ming-Hsuan Yang<sup>5</sup></a></td>
		            </tr>
		        </table>
		        <table>
					<tr>
			          <td height="20" colspan="4" align='center'><sup>1</sup>Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
		            <tr>
			          <td height="20" colspan="4" align='center'><sup>2</sup>School of Computer Science and Technology, Huazhong University of
Science and Technology, Hubei, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
			    <tr>
			          <td height="20" colspan="4" align='center'><sup>3</sup>Yunnan Open University, Yunnan, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
			    <tr>
			          <td height="20" colspan="4" align='center'><sup>4</sup>Moshanghua Tech Co. Ltd., Beijing, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
		            <tr>
			          <td height="20" colspan="4" align='center'><sup>5</sup>University of California at Merced, Merced, CA, USA.</td>
			          <p align="center">
			            </p>
		            </tr>
		          </table>
	        </div>
          </div>
		<div class="nline1"></div>
		</div>
	</div>



<!-- --------------------------------
-
- Abstract
-
---- --------------------------------
-->        
    <div id="cont">
		<div class="wrap">        	
	    	<h2 id="subject">Abstract</h1>
            <p align="justify" style="text-indent:2em">
Image retargeting has been applied to display images 
of any size via devices with various resolutions (e.g., cell 
phone, TV monitors). To fit an image with the target resolution, 
certain unimportant regions need to be deleted or distorted and 
the key problem is to determine the importance of each pixel. 
Existing methods predict pixel-wise importance in a bottom-up 
manner via eye fixation estimation or saliency detection. 
In contrast, the proposed algorithm estimates the pixel-wise 
importance based on a top-down criterion where the target image 
maintains the semantic meaning of the original image. To this 
end, several semantic components corresponding to foreground 
objects, action contexts, and background regions are extracted. 
The semantic component maps are integrated by a classification 
guided fusion network. Specifically, the deep network classifies 
the original image as object or scene-oriented, and fuses the 
semantic component maps according to classification results. The 
network output, referred to as the semantic collage with the 
same size as the original image, is then fed into any existing 
optimization method to generate the target image. Extensive 
experiments are carried out on the <I>RetargetMe</I> dataset and <I>S-Retarget</I> 
database developed in this work. Experimental results 
demonstrate the merits of the proposed algorithm over the state-of- 
the-art image retargeting methods. 
            </p>
			<div class="line"></div>
      </div>
    </div>

<!-- --------------------------------
-
- The S-Retarget dataset
-
---- --------------------------------
-->
<div id="cont">
		<div class="wrap">        	
	    	<h2 id="subject">The S-Retarget Dataset</h2>
	    	<p align="justify" style="text-indent:2em"> 
For comprehensive evaluation of image
retargeting methods, we construct the Semantic-Retarget (<I>S-
Retarget</I>) dataset which contains 1, 527 images.
	    	</p>
	    	<p align="justify" style="text-indent:2em"> 
We select images from some existing datasets as well as some pictures from web search engines.
Based on the
contents, all images are divided into 6 categories including
single person, multiple people, single as well as multiple
objects, and indoor as well as outdoor scenes. 

To give each images their semantic collages, we ask 5 subjects to annotate the pixel
relevance score based on the semantics of an image.  Figure 1(b) demonstrates the
semantic collage marked by two annotators. Overall, the annotations
from different persons are consistent. The relevance
score of one pixel is obtained by averaging all relevance scores
of the segments covering the pixel (see Figure 1(c)).
	    	</p>
	    	<p style="text-align:center">
         		<img src="images/SPR/annotation_extended_1.png" alt="" width="800" height="461" align="bottom" />
         	</p>
         	<div class="caption">
            	<p class="caption-content">
               		<strong class="fig-label">Figure 1</strong>. 
Some examples of semantic collage in the S-Retarget dataset. a) original images; b) annotations from two annotators; c) ground truth annotations; d)
image caption.
            	</p>
		<p align="justify" style="text-indent:2em"> 
The <I>S-Retarget</I> dataset can also be used as a semantic
saliency dataset. Different with the saliency datasets, e.g.,
MSRA-5000<sup>[3]</sup> or ECSSD<sup>[4]</sup>, which mainly contain
dominant objects, the images in <I>S-Retarget</I> are quite diverse.
Furthermore, as shown in Figure 1(c), the ground truth are
labeled with soft rather than binary annotations. 
		</p>
         	</div>
	    	<h3 id="subject">Dataset Download [620MB]</h3>
	    	<p align="justify" style="text-indent:2em"> 
	    		<a href="http://pan.baidu.com/s/1eSC9xLo" target="_blank">Baidu Yun</a> 
		</p>
		<p align="justify" style="text-indent:2em"> 
			<a href="https://my.pcloud.com/publink/show?code=XZHoglZSatxXLzrKS7iIleNtRPPBYvJ606V" target="_blank">pCloud</a> 
	    	</p>
	    	<div class="line"></div>
        </div>
</div>


<!-- --------------------------------
-
- Architecture Overview
-
---- --------------------------------
-->
   <div id="cont">
      <div class="wrap">
         <h2 id="subject">Architecture Overview</h2>
         <p align="justify" style="text-indent:2em"> 
Although the state-of-the-art
modules are used, semantic components may not be extracted
well in an image. Thus, we combine all the semantic component
maps via a classification guided fusion network to generate
the semantic collage. As object and scene images have
different properties, the fusion network first classifies
an image into two types. The semantic component maps are
then fused by the corresponding sub-network based on the
specified category. In contrast to existing methods, we exploit
the semantic collages based on three defined components for
image retargeting. The generated semantic collage is fed into a
carrier method to generate the target image.
		</p>
         <p style="text-align:center">
         	<img src="images/SPR/framework_1.png" alt="" width="800" height="282" align="bottom" />
         </p>
         <div class="caption">
            <p class="caption-content">
               <strong class="fig-label">Figure 1</strong>. 
Main steps of the SP-DIR algorithm. The semantic meaning of the original image is: a boy kicks a ball on a pitch. Three semantic components
including boy, ball, kick and pitch are extracted first. These are fused via a classification guided fusion network to generate a semantic collage, which is fed
into the carrier to render the target image.
            </p>
         </div>
         <p style="text-align:center">
         	<img src="images/SPR/fusion_network.png" alt="" width="850" height="280" align="bottom" />
         </p>
         <div class="caption">
            <p class="caption-content">
               <strong class="fig-label">Figure 2</strong>. 
Classification guided fusion network. The inputs are the multiple semantic component maps and the output is the semantic collage. The classification
 sub-network predicts the image as either scene or object oriented. The the regression sub-network fuses the semantic components maps
 according to the classification results.
            </p>
        </div>
		<div class="line"></div>
      </div>
   </div>      

	<div id="footer">
    
    </div>
      	<div id="cont">
     	<div class="wrap">
         <h2 id="subject">Performance</h2>
         <h3 id="subject">Quantified Evaluation</h3>
        	<p align="justify"  style="text-indent:2em">
				We compare our importance maps with state-of-the-art saliency
				generating methods under the four evaluation criteria of the MIT saliency
 				benchmark<sup>[5]</sup> and the Mean Average Error (MAE). For EMD, KL, MAE, the lower
 				 the better while for CC and SIM, the higher the better.
			</p>
        	<p align="justify"  style="text-indent:2em">
				It can be observed that our fusion network performs best on
				 the S-Retarget dataset under all evaluation metrics.
         	</p>
 		 <p align="center">
 		 	<strong class="fig-label">Table 1. </strong>
			Evaluation on importance maps on the validation-set in the <I>S-Retarget</I> dataset.
		  <img src="images/SPR/table_importance_map_baseline_1.png" alt="" width="500" height="334" align="bottom" />
		 </p>
		 <h3 id="subject">Retargeting Results</h3>
		 	<p align="justify"  style="text-indent:2em">
				We applied our system on S-Retarget dataset as well as the RetargetMe<sup>[1]</sup> dataset. The following retargeting
				results show that our method can better preserve the semantic meanings in images.
         	</p>
         	<p align="center">
		  		<img src="images/SPR/system_compare2.png" alt="" width="800" height="677" align="bottom" />
		    </p>
		    <div class="caption">
            	<p class="caption-content">
            		<strong class="fig-label">Figure 3</strong>. 
		  				Comparisons with SOAT, ISC, Multi-operator, Warp, AAD , OSS on S-Retarget dataset. 6 rows show the results for single person,
multiple people, single object, multiple objects, indoor scene and outdoor scene, respectively
		 	    </p>
        	</div>
		 	<p align="center">
		  		<img src="images/SPR/figure12-2.png" alt="" width="800" height="365" align="bottom" />
		  	</p>
		  	<div class="caption">
            	<p class="caption-content">
            		<strong class="fig-label">Figure 4</strong>. 
		  				Results on RetargetMe dataset. Target images are obtained by using 3 retargeting methods (AAD, Multi-Op, and IF, and 9 importance
maps ( eDN, GC, oriIF, DNEF, RCC, fine-tuned MC, fine-tuned Mr-cnn, fine-tuned SalNet and our method).
		 		</p>
        	</div>
         	<p align="justify"  style="text-indent:2em">
				We also conducted human evaluations on the Amazon Mechanical Turk (AMT). Our target image and the result
				 by a baseline are shown in randomly order to the AMT workers, who are asked to select the better one.
				The evaluation results are shown below. Each element stands for a contrastive result, for example, the
				number “2985(255)” means our result is preferred by 2,985 times while the corresponding baseline method
				 is favored by 255 times.
         	</p>
         	<p align="center">
         		<strong class="fig-label">Table 2. </strong>
				Comparison between our importance map and 8 baseline maps when combined with 3 different carriers on S-Retarget dataset.
		  		<img src="images/SPR/Table2.png" alt="" width="600" height="548" align="bottom" />
		 	</p>
		 	<p align="center">
		 		<strong class="fig-label">Table 3. </strong>
				Comparisons with state-of-the-art retargeting systems on S-Retarget dataset.
		  		<img src="images/SPR/Table3.png" alt="" width="500" height="48" align="bottom" />
		 	</p>
		 	<p align="center">
		 		<strong class="fig-label">Table 4. </strong>
				Comparison between our importance map and 8 baseline maps when combined with 3 carriers on RetargetMe dataset.
		  		<img src="images/SPR/Table4.png" alt="" width="600" height="101" align="bottom" />
		 	</p>
		<a href="http://www.spretarget.com/supp_.html"><h3 id="subject">Click here for more visualization and analysis</h3></a>
		 <p align="center">
		  	<img src="images/SPR/supp_.png" alt="" width="600" height="101" align="bottom" />
		 </p>
		<div class="line"></div>
	</div>

<!-- --------------------------------
-
- References
-
---- --------------------------------
-->
  	<div id="cont">
     	<div class="wrap">
         <h2 id="subject">References</h2>
         <ul class="ref-list" align="justify">
	    <li class="ref-item">M. Rubinstein, D. Gutierrez, O. Sorkine, and A. Shamir, A comparative study of image retargeting, in ACM TOG, 2010. </li>
	    <li class="ref-item">A. Mansfield, P. Gehler, L. Van Gool, and C. Rother, Visibility maps for improving seam carving, in ECCV Workshops, 2010. </li>
	    <li class="ref-item">T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, Learning to detect a salient object, in CVPR, 2007. </li>
	    <li class="ref-item">Q. Yan, L. Xu, J. Shi, and J. Jia, Hierarchical saliency detection, in CVPR, 2013. </li>
            <li class="ref-item">Z. Bylinskii, T. Judd, A. Borji, L. Itti, F. Durand, A. Oliva and A. Torralba. MIT Saliency Benchmark. <a href="http://saliency.mit.edu/">http://saliency.mit.edu/</a>.</li>
         </ul>
      </div>
 	</div>

 	<div class="line"></div>
 		<div align="center">
 			<table width="40%" border="0" align="center" cellpadding="0" cellspacing="0">
				<tr>
					<td width="25%" height="30" align='center'><font size="2" color="gray">2016, by Zhen Wei<font></td>
					<td width="25%" height="30" align='center'>
						<a href="https://clustrmaps.com/site/18rtx" title="Visit tracker">
							<img src="//www.clustrmaps.com/map_v2.png?d=MV4_6ipBWn-Yyl5u8r0hQx5XcO2WBPFvux4Q5D6UHYY&cl=ffffff" />
						</a>
 					</td>
				</tr>
			</table>
		</div>
</body>
</html>
