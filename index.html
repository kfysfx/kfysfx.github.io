<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="Video Event Detection" />
	<meta name="keywords" content="Constraint Flow" />	
	<meta name="author" content="Wei Zhen" />
	<link rel="stylesheet" href="css/SPR.css" type="text/css" />
	<title>Semantic Preserving Retargeting</title>
</head>


<body>


<!-- --------------------------------
-
- header
-
---- --------------------------------
-->  
	<div id="header">
		<div class="wrap">
		  <div id="intro">
		    	<h1 align="center" id="logo">Semantic Preserving Retargeting</h1>
		      <div align="center">
			      <table width="80%" border="0" align="center" cellpadding="0" cellspacing="0">
			        <tr>
			          <td width="25%" height="30" align='center'><a target="_blank">Si Liu<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Zhen Wei<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Yao Sun<sup>1</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Xinyu Ou<sup>1,2,3</sup></a></td>
					  <td width="20%" height="30" align='center'><a target="_blank">Junyu Lin<sup>1</sup></a></td>
		            </tr>
		        </table>
		        <table>
		            <tr>
		              <td width="25%" height="30" align='center'><a target="_blank">Bin Liu<sup>4</sup></a></td>
					  <td width="25%" height="30" align='center'><a target="_blank">Ming-Hsuan Yang<sup>5</sup></a></td>
		            </tr>
		        </table>
		        <table>
					<tr>
			          <td height="20" colspan="4" align='center'><sup>1</sup>Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
		            <tr>
			          <td height="20" colspan="4" align='center'><sup>2</sup>School of Computer Science and Technology, Huazhong University of
Science and Technology, Hubei, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
			    <tr>
			          <td height="20" colspan="4" align='center'><sup>3</sup>Yunnan Open University, Yunnan, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
			    <tr>
			          <td height="20" colspan="4" align='center'><sup>4</sup>Moshanghua Tech Co. Ltd., Beijing, P.R.China</td>
			          <p align="center">
			            </p>
		            </tr>
		            <tr>
			          <td height="20" colspan="4" align='center'><sup>5</sup>University of California at Merced, Merced, CA, USA.</td>
			          <p align="center">
			            </p>
		            </tr>
		          </table>
	        </div>
          </div>
		<div class="nline1"></div>
		</div>
	</div>



<!-- --------------------------------
-
- Abstract
-
---- --------------------------------
-->        
    <div id="cont">
		<div class="wrap">        	
	    	<h2 id="subject">Abstract</h1>
            <p align="justify" style="text-indent:2em">
Image retargeting has been applied to display images 
of any size via devices with various resolutions (e.g., cell 
phone, TV monitors). To fit an image with the target resolution, 
certain unimportant regions need to be deleted or distorted and 
the key problem is to determine the importance of each pixel. 
Existing methods predict pixel-wise importance in a bottom-up 
manner via eye fixation estimation or saliency detection. 
In contrast, the proposed algorithm estimates the pixel-wise 
importance based on a top-down criterion where the target image 
maintains the semantic meaning of the original image. To this 
end, several semantic components corresponding to foreground 
objects, action contexts, and background regions are extracted. 
The semantic component maps are integrated by a classification 
guided fusion network. Specifically, the deep network classifies 
the original image as object or scene-oriented, and fuses the 
semantic component maps according to classification results. The 
network output, referred to as the semantic collage with the 
same size as the original image, is then fed into any existing 
optimization method to generate the target image. Extensive 
experiments are carried out on the <I>RetargetMe</I> dataset and <I>S-Retarget</I> 
database developed in this work. Experimental results 
demonstrate the merits of the proposed algorithm over the state-of- 
the-art image retargeting methods. 
            </p>
			<div class="line"></div>
      </div>
    </div>

<!-- --------------------------------
-
- The S-Retarget dataset
-
---- --------------------------------
-->
<div id="cont">
		<div class="wrap">        	
	    	<h2 id="subject">The S-Retarget Dataset</h2>
	    	<p align="justify" style="text-indent:2em"> 
Several retargeting benchmark datasets have been developed
including the <I>RetargetMe</I><sup>[1]</sup> dataset and the one collected
by <I>Mansfield et al.</I><sup>[2]</sup>. However, these datasets contains only
80 or 100 images. For comprehensive evaluation of image
retargeting methods, we construct the Semantic-Retarget (<I>S-
Retarget</I>) dataset which contains 1, 527 images.
	    	</p>
	    	<h3 id="subject">Image collection</h3>
	    	<p align="justify" style="text-indent:2em"> 
We select images from the Pascal VOC<sup>[3]</sup>,
SUN<sup>[4]</sup>, and BSR<sup>[5]</sup> datasets. In addition, we collect
images from Google and Bing search engines. Based on the
contents, all images are divided into 6 categories including
single person, multiple people, single as well as multiple
objects, and indoor as well as outdoor scenes. The images
in single person, multiple people, single object and multiple
objects classes are object-oriented while other images are
scene-oriented. The dataset is split into train/val/test subsets, containing 1, 237,
145 and 145 images respectively. The distribution of the 6
categories are almost the same in the three sets.
	    	</p>
	    	<h3 id="subject">Semantic collage</h3>
	    	<p align="justify" style="text-indent:2em"> 
WWe ask 5 subjects to annotate the pixel
relevance based on the semantics of an image. The labeling
process has two stages. In the first stage, all annotators
explicitly record the semantic meaning of the images. Several
image captions are shown in Figure 1(d). In the second stage,
the annotators rate all pixels according to the relevances to
the image caption. To facilitate labeling, each image is oversegmented
5 times using multiple over-segmentations methods
including SLIC<sup>[6]</sup> 3 times and Quick Shift<sup>[7]</sup> 2 twice with
different segmentation parameters, e.g., number of superpixels
and compactness factors. Then each annotator assigns a value
to each image segment where a higher score corresponds to
high relevance. For ease of labeling, the relevance score can
only take three values, i.e., 0 (irrelevant), 0.5 (moderately
relevant) and 1 (high relevant). Figure 1(b) demonstrates the
semantic collage marked by two annotators. Overall, the annotations
from different persons are consistent. The relevance
score of one pixel is obtained by averaging all relevance scores
of the segments covering the pixel. Several examples of the
ground truth maps are shown in Figure 1(c).
	    	</p>
	    	<p style="text-align:center">
         		<img src="images/SPR/annotation_extended.png" alt="" width="800" height="461" align="bottom" />
         	</p>
         	<div class="caption">
            	<p class="caption-content">
               		<strong class="fig-label">Figure 1</strong>. 
Some examples of semantic collage in the S-Retarget dataset. a) original images; b) annotations from two annotators; c) ground truth annotations; d)
image caption.
            	</p>
		<p align="justify" style="text-indent:2em"> 
The <I>S-Retarget</I> dataset can also be used as a semantic
saliency dataset. Different with the saliency datasets, e.g.,
MSRA-5000<sup>[8]</sup> or ECSSD<sup>[9]</sup>, which mainly contain
dominant objects, the images in <I>S-Retarget</I> are quite diverse.
Furthermore, as shown in Figure 1(c), the ground truth are
labeled with soft rather than binary annotations. 
		</p>
         	</div>
	    	<h3 id="subject">Dataset Download [620MB]</h3>
	    	<p align="justify" style="text-indent:2em"> 
	    		<a href="http://pan.baidu.com/s/1eSC9xLo" target="_blank">Baidu Yun</a> 
		</p>
		<p align="justify" style="text-indent:2em"> 
			<a href="https://my.pcloud.com/publink/show?code=XZHoglZSatxXLzrKS7iIleNtRPPBYvJ606V" target="_blank">pCloud</a> 
	    	</p>
	    	<div class="line"></div>
        </div>
</div>


<!-- --------------------------------
-
- Architecture Overview
-
---- --------------------------------
-->
   <div id="cont">
      <div class="wrap">
         <h2 id="subject">Architecture Overview</h2>
         <p align="justify" style="text-indent:2em"> 
         	The goal of SPR is that the target image keeps
the global semantics as much as possible. Technically, we
need to generate a global importance map assigning higher
weights to the pixels expressing the global semantics.
		</p>
		<p align="justify" style="text-indent:2em">
         	Generating a global importance map is composed of two steps:
1) decomposing global semantics into several local semantics. 2) fusing 
the local semantics induced importance maps into a global importance
map.The two processes are shown in the following two figures.
         </p>
         <p style="text-align:center">
         	<img src="images/SPR/framework.png" alt="" width="800" height="282" align="bottom" />
         </p>
         <div class="caption">
            <p class="caption-content">
               <strong class="fig-label">Figure 1</strong>. 
The framework of generating local importance maps.
            </p>
         </div>
         <p style="text-align:center">
         	<img src="images/SPR/fusion_network.png" alt="" width="850" height="280" align="bottom" />
         </p>
         <div class="caption">
            <p class="caption-content">
               <strong class="fig-label">Figure 2</strong>. 
The fusion network to synthesize global importance map.
            </p>
        </div>
        <p align="justify" style="text-indent:2em"> 
		 	The fused importance map, which captures the
		 	context among local semantics and has higher weights
			in the pixels corresponding to the global semantics, is fed
			into any subsequent target image generation methods.
		 </p>
		<div class="line"></div>
      </div>
   </div>      

	<div id="footer">
    
    </div>
      	<div id="cont">
     	<div class="wrap">
         <h2 id="subject">Performance</h2>
         <h3 id="subject">Quantified Evaluation</h3>
        	<p align="justify"  style="text-indent:2em">
				We compare our importance maps with state-of-the-art saliency
				generating methods under the four evaluation criteria of the MIT saliency
 				benchmark<sup>[1]</sup> and the Mean Average Error (MAE). For EMD, KL, MAE, the lower
 				 the better while for CC and SIM, the higher the better.
			</p>
        	<p align="justify"  style="text-indent:2em">
				It can be observed that our fusion network performs best on
				 the S-Retarget dataset under all evaluation metrics.
         	</p>
 		 <p align="center">
 		 	<strong class="fig-label">Table 1. </strong>
			Quantified evaluation of importance map regression on the validation-set in S-Retarget dataset.
		  <img src="images/SPR/table_importance_map_baseline.png" alt="" width="500" height="334" align="bottom" />
		 </p>
		 <h3 id="subject">Retargeting Results</h3>
		 	<p align="justify"  style="text-indent:2em">
				We applied our system on S-Retarget dataset as well as the RetargetMe<sup>[2]</sup> dataset. The following retargeting
				results show that our method can better preserve the semantic meanings in images.
         	</p>
         	<p align="center">
		  		<img src="images/SPR/system_compare2.png" alt="" width="800" height="677" align="bottom" />
		    </p>
		    <div class="caption">
            	<p class="caption-content">
            		<strong class="fig-label">Figure 3</strong>. 
		  				Comparisons with SOAT, ISC, Multi-operator, Warp, AAD , OSS on S-Retarget dataset. 6 rows show the results for single person,
multiple people, single object, multiple objects, indoor scene and outdoor scene, respectively
		 	    </p>
        	</div>
		 	<p align="center">
		  		<img src="images/SPR/figure12-2.png" alt="" width="800" height="365" align="bottom" />
		  	</p>
		  	<div class="caption">
            	<p class="caption-content">
            		<strong class="fig-label">Figure 4</strong>. 
		  				Results on RetargetMe dataset. Target images are obtained by using 3 retargeting methods (AAD, Multi-Op, and IF, and 9 importance
maps ( eDN, GC, oriIF, DNEF, RCC, fine-tuned MC, fine-tuned Mr-cnn, fine-tuned SalNet and our method).
		 		</p>
        	</div>
         	<p align="justify"  style="text-indent:2em">
				We also conducted human evaluations on the Amazon Mechanical Turk (AMT). Our target image and the result
				 by a baseline are shown in randomly order to the AMT workers, who are asked to select the better one.
				The evaluation results are shown below. Each element stands for a contrastive result, for example, the
				number “2985(255)” means our result is preferred by 2,985 times while the corresponding baseline method
				 is favored by 255 times.
         	</p>
         	<p align="center">
         		<strong class="fig-label">Table 2. </strong>
				Comparison between our importance map and 8 baseline maps when combined with 3 different carriers on S-Retarget dataset.
		  		<img src="images/SPR/Table2.png" alt="" width="600" height="548" align="bottom" />
		 	</p>
		 	<p align="center">
		 		<strong class="fig-label">Table 3. </strong>
				Comparisons with state-of-the-art retargeting systems on S-Retarget dataset.
		  		<img src="images/SPR/Table3.png" alt="" width="500" height="48" align="bottom" />
		 	</p>
		 	<p align="center">
		 		<strong class="fig-label">Table 4. </strong>
				Comparison between our importance map and 8 baseline maps when combined with 3 carriers on RetargetMe dataset.
		  		<img src="images/SPR/Table4.png" alt="" width="600" height="101" align="bottom" />
		 	</p>
		<div class="line"></div>
	</div>


<!-- --------------------------------
-
- Paper
-
---- --------------------------------
-->
<!-------------
<div id="cont">
		<div class="wrap">        	
	    	<h2 id="subject">Paper</h2>
			<div class="paper-info-box" align="center">
				<div class="paper-title-box">
					Semantic Preserving Retargeting
				</div>
				<div class="paper-author-box">
 					Si Liu, Zhen Wei, Yao Sun, Xinyu Ou, Jizhong Han, Ming-Hsuan Yang
				</div>
			</div>
		      <div align="center">
			      <table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
			        <tr>
			          <td width="18%" align='center'> <a target="_blank"> <div align="center"> <img src='images/SPR/paper.png' width="140" height="198"></div></a></td>
                      <td width="82%">
                           <pre>
                              <code>
@inproceedings{liu2016Semantic,
  title={Semantic Preserving Retargeting},
  author={Liu, Si and Zhen, Wei and Yao, Sun and Xinyu Ou and Jizhong Han and Ming-Hsuan Yang},
  journal = {arXiv preprint arXiv:1605.xxxxxx},
  year={2016}
}
                                </code>
                            </pre>
                       </td>
		            </tr>
			        <tr>
			          <td width="19%" align='center'> <a target="_blank">[arxiv preprint]</a></td>
		            </tr>
		          </table>
	        </div>
			<div class="line"></div>
      </div>
      </div>
-->

<!-- --------------------------------
-
- References
-
---- --------------------------------
-->
  	<div id="cont">
     	<div class="wrap">
         <h2 id="subject">References</h2>
         <ul class="ref-list" align="justify">
	    <li class="ref-item">M. Rubinstein, D. Gutierrez, O. Sorkine, and A. Shamir, A comparative study of image retargeting, in ACM TOG, 2010. </li>
	    <li class="ref-item">A. Mansfield, P. Gehler, L. Van Gool, and C. Rother, Visibility maps for improving seam carving, in ECCV Workshops, 2010. </li>
	    <li class="ref-item">M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, The pascal visual object classes (voc) challenge, in IJCV, 2010. </li>
	    <li class="ref-item">J. Xiao, K. Ehinger, J. Hays, A. Torralba, and A. Oliva, Sun database: Exploring a large collection of scene categories, in IJCV, 2014. </li>
	    <li class="ref-item">D. Martin, C. Fowlkes, D. Tal, and J. Malik, A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics, in ICCV, 2001. </li>
	    <li class="ref-item">R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, Slic superpixels compared to state-of-the-art superpixel methods, in TPAMI, 2012. </li>
	    <li class="ref-item">A. Vedaldi and S. Soatto, Quick shift and kernel methods for mode seeking, in ECCV, 2008. </li>
	    <li class="ref-item">T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, Learning to detect a salient object, in CVPR, 2007. </li>
	    <li class="ref-item">Q. Yan, L. Xu, J. Shi, and J. Jia, Hierarchical saliency detection, in CVPR, 2013. </li>
            <li class="ref-item">Z. Bylinskii, T. Judd, A. Borji, L. Itti, F. Durand, A. Oliva and A. Torralba. MIT Saliency Benchmark. <a href="http://saliency.mit.edu/">http://saliency.mit.edu/</a>.</li>
            <li class="ref-item">R. Michael, G. Diego, S. Olga and S. Ariel. A comparative study of image retargeting. In TOG, 2010.</li>
         </ul>
      </div>
 	</div>

 	<div class="line"></div>
 		<div align="center">
 			<table width="40%" border="0" align="center" cellpadding="0" cellspacing="0">
				<tr>
					<td width="25%" height="30" align='center'><font size="2" color="gray">2016, by Zhen Wei<font></td>
					<td width="25%" height="30" align='center'>
						<a href="https://clustrmaps.com/site/18rtx" title="Visit tracker">
							<img src="//www.clustrmaps.com/map_v2.png?d=MV4_6ipBWn-Yyl5u8r0hQx5XcO2WBPFvux4Q5D6UHYY&cl=ffffff" />
						</a>
 					</td>
				</tr>
			</table>
		</div>
</body>
</html>
